
shuffling
executor 1 - [(1, "alice"), (2, "bob"), (1, "charlie")]
executor 2 - [(3, "daniel"), (1, "superman")]
executor 3 - [(4, "wolverine")]

step 1 - define a hashing function - which row stays on which partition
  hash function = department % num_executors
  num_executors always known (and configurable before starting the job)
step 2 - hash function sent to all executors (by the driver)
step 3 - create the new partitions
  executor 1: partition 1 - [(1,"alice"), (1, "charlie"), (1, "superman") (4, "wolverine")]
  executor 2: partition 2 - [(2, "bob")]
  executor 3: partition 3 - [(3, "daniel")]
step 4 - perform the aggregation in parallel


example with partial result
groupBy(department).sum(coins) - wide transformation

executor 1 - [(1, "alice", 1), (2, "bob", 3), (1, "charlie", 2)]
executor 2 - [(3, "daniel", 0), (1, "superman", 99)]
executor 3 - [(4, "wolverine", 10)]
sum all the coins per department

step 1 - do this locally - partial results
  executor 1 - [(1, 3 coins), (2, 3 coins)]
  executor 2 - [(3, 0 coins), (1, 99 coins)]
  executor 3 - [(4, 10 coins)]
step 2 - hash function - department % num_executors
step 3 - send the hash function to all executors
step 4 - create the new partitions
  executor 1 - [(1, 3 coins),(1, 99 coins), (4, 10 coins)]
  executor 2 - [(2, 3 coins)]
  executor 3 - [(3, 0 coins)]
step 5 - combine the partial results into a big result
  executor 1 - [(1, 102 coins), (4, 10 coins)]
  executor 2 - [(2, 3 coins)]
  executor 3 - [(3, 0 coins)]